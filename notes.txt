---------------------------------------------------------
                    DESCRIPTIVES
----------------------------------------------------------

Tips for report:
1. Only include things if they relate to the whole
2. Describe what we tried to determine features: "we tried to see whether part-of-speech tags are of interest for
prediction, yet there were no interesting differences. It seems that people use the same words to express a negative
review and a positive review.

- General
file = trainset-sentiment.csv # Please use this file, the reviews are almost the same, but the trainset-sentiment-extra
contains some weird words in the reviews.

Describe the columns and especially the labels
Reviews about different things.

- Frequencies
We used vocabulary size
We looked at the most frequent words (also compared without stopwords)
Tokenizer: nltk word tokenizer (what about wordpunct tokenizer

- Collocations / most frequent bigrams

- Part of Speech Tagging:
* It takes too much time to do part-of-speech tagging. The files are too big. Therefore, I used pickle files.
I created two pickle files: one with all the positive label texts and one with all the negative label texts. It's important to use
this because we only need to create these files once and then we can use them for anything we want. Pickle files are
different then just normal files, I'm not sure how it works but it sterilises a file. Anyway, it's really faster than
normal files.
* I looked at: verbs, nouns and adjectives. Please have a look at the POSTAGS.txt file to understand all the
abbrevations / tags. I looked at all types of nouns and all types of verbs. Yet, they are not very informative

Motivation:
Twofold:
1. Part-of-speech, collocations are of interest to predict the labels.
2. Part-of-speech, collocations tell us something about the corpus.

--------------------------------------------------------------
                    INFERENTIAL
------------------------------------------------------------

RQ:
Hypothesis:

Tip for report:
* we can determine that based on the data itself (bottom-up) or
based on literature (top-down). We use bottom-up.
Mention this in the report!!
* we describe things about the data (collocations, part-of-speech, bigrams). We saw some interesting
things concerning bigrams.

---------------------------------------------------------
                     PREDICTION
----------------------------------------------------------
For the prediction part:

We can add our own features. The only thing we need to do is: add a column. For instance:

"We want to check positive words and negative words. We have a list with positive words and
negative words. Positive words are: 'Like, Happy' and Negative words are 'Sad, Angry'. We want
to determine whether a positive review has more positive words."
Step 1: add the number of positive words. Use a python script.
Step 2: run Malvina's command line prediction thingy. Ad the name of the column
to the command. So you use:
--- features NUMBER-POS-WORDS

LABEL   TEXT                            NUMBER-POS-WORDS
POS     I really like this book!        1

We already saw some interesting things with bigrams, we can also use that:

--nwords 2(looks at bigrams)

We also saw some interesting things concerning the length (lexical density), we can add length as a feature.
